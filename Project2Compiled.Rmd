---
title: "Project2Compiled"
author: "Evan Fisher"
date: "April 15, 2017"
output:   
  html_document: 
    theme: united
    toc: yes
    number_sections: TRUE
    toc_float:
      collapsed: TRUE
---
#Introduction

```{r, include = F}

data1 = read.csv("sdclean.csv", header = T)

```

###Explanation:
```{}
Our team cleaned the data using the provided data cleaning script. The file being read in has already been cleaned on a basic level.
```


##Cleaning Data

```{r}
#who will match on this dataset
train1 = data1[1:5000,]
test1 = data1[5001:6000,]

train2 = data1[1:5000,]
test2 = data1[5001:6000,]
#tele_nn <- model.matrix(~ job + marital + education + default + housing + loan +  contact + month + day_of_week + poutcome + y, data1 = tele1)
```

###Explanation:
```{}
Here we created test/train dataframes.
```


#Building a Logistic Regression Model

###Our Process:
```{}
Our team began our analysis using a logistic regression model. As seen below, we began with a basic model and refined our variables down to exclusively the most signficant factors through an iterative process. Ultimately, we determined int_corr, age_o, pf_o_sin, attr_o, fun_o, prob_o, met_o, imprace, exercise, gaming, concerts, exphappy, and intel2_1 to be the most signficant determinants. Please see below for a step-by-step process.
```

##Basic Model - All Variables

```{}
The model below is the first and most basic of our logistic regressions. To begin, we ran a logistic regression on all possible variables in order to determine which are significant. The significant variables, according to this logistic regression, are gender, int_corr, age_o, samerace, race_o, pf_o_sin, pf_o_sha, attr_o, fun_o, amb_o, prob_o, met_o, race, goal, career_c, imprace, sports, exercise, art, gaming, clubbing, tv, theater, movies, concerts, yoga, exphappy, attr2_1, sinc2_1, intel2_1, fun2_1, amb2_1, and shar2_1. Within this model, we only required 90% confidence to define a variable as significant.
```

```{r}
log_all1 = glm(match ~ ., data = train2, family = "binomial")
summary(log_all1)
```

##90% Confidence Interval Regression (Workdown)

```{}
From the original base model, we identified the variables that held signficant with 90% confidence and included them in the next iteration of our model (see below).
```

```{r}
log_all3 = glm(match ~ gender + int_corr + age_o + samerace + race_o + pf_o_sin + pf_o_sha + attr_o + fun_o + amb_o + prob_o + met_o + race + goal + career_c + imprace + sports + exercise + art + gaming + clubbing + tv + theater + movies + concerts + yoga + exphappy + attr2_1 + sinc2_1 + intel2_1 + fun2_1 + amb2_1 + shar2_1, data = train2, family = "binomial")
summary(log_all3)
```

```{}
Our team then identified significant variables from the basic model and created a new logistic regression with exclusively the aforementioned variables. From this regression, we identified which variables are more significant than others. Through our analysis, we decided to eliminate gender, samerace, race_o, race, sports, art, tv, theater, yoga, fun2_1, amb2_1, shar2_1 from the next iteration of our model (seen below).
```

```{r}
log_all4 = glm(match ~ int_corr + age_o + pf_o_sin + pf_o_sha + attr_o + fun_o + amb_o + prob_o + met_o + goal + career_c + imprace + exercise + gaming + clubbing + movies + concerts + exphappy + attr2_1 + sinc2_1 + intel2_1, data = train2, family = "binomial")
summary(log_all4)
```

```{}
In the next iteration of our model, we again determined which variables are the most signficant. After this round we deleted sinc2_1.
```

```{r}
log_all5 = glm(match ~ int_corr + age_o + pf_o_sin + pf_o_sha + attr_o + fun_o + amb_o + prob_o + met_o + goal + career_c + imprace + exercise + gaming + clubbing + movies + concerts + exphappy + attr2_1 + intel2_1, data = train2, family = "binomial")
summary(log_all5)
```

```{}
Once again, we ran the model and determined which variables are best. We eliminated clubbing after the above model.
```

```{r}
log_all6 = glm(match ~ int_corr + age_o + pf_o_sin + pf_o_sha + attr_o + fun_o + amb_o + prob_o + met_o + goal + career_c + imprace + exercise + gaming + movies + concerts + exphappy + attr2_1 + intel2_1, data = train2, family = "binomial")
summary(log_all6)
```

```{}
After this model, we determined that goal is less significant than the other variables.
```

```{r}
log_all7 = glm(match ~ int_corr + age_o + pf_o_sin + pf_o_sha + attr_o + fun_o + amb_o + prob_o + met_o + career_c + imprace + exercise + gaming + movies + concerts + exphappy + attr2_1 + intel2_1, data = train2, family = "binomial")
summary(log_all7)
```

```{}
After this model, we removed movies and attr2_1.
```

```{r}
log_all8 = glm(match ~ int_corr + age_o + pf_o_sin + pf_o_sha + attr_o + fun_o + amb_o + prob_o + met_o + career_c + imprace + exercise + gaming + concerts + exphappy + intel2_1, data = train2, family = "binomial")
summary(log_all8)
```

```{}
The above model was our final logistic model. The variables used were int_corr, age_o, pf_o_sin, pf_o_sha, attr_o, fun_o, amb_o, prob_o, met_o, career_c, imprace, exercise, gaming, concerts, exphappy, and intel2_1. From this, we can determine that these variables are the most signficant and therefore are the most important determinants of two potential partners matching.
```

##Predictions from Logistic Models (all variables)

```{r}
# Predict the Match Probability for the Testing Dataset using the model developed on the training dataset
#use survival.model2
predict.model.log_all8 = predict(log_all8, newdata = test2, type = "response")
predict.model.log_all8 = as.data.frame(predict.model.log_all8, na.rm = TRUE)
predict.model.log_all8$predict.model <- as.numeric(predict.model.log_all8$predict.model)
predict.model.log_all8$match <- ifelse(predict.model.log_all8$predict.model > 0.5, 1, 0)
predict.model.log_all8$X <- test2$X
predict.model.log_all8
prop.table(table(predict.model.log_all8$match))
```

##Takeaway
```{}
Using our refined logistic regression, we can predict the likelihood of two individuals matching. The most important factors in crafting this prediction are int_corr, age_o, pf_o_sin, pf_o_sha, attr_o, fun_o, amb_o, prob_o, met_o, career_c, imprace, exercise, gaming, concerts, exphappy, and intel2_1.
```

#SVM

##Clean Data

```{r}
#getting the numerical variables
str(data1)
num = sapply(data1, is.numeric)
nums = data1[,num]
train1 = data1[1:5000,]
test1 = data1[5001:6000,]
```

```{}
For the SVM model, we needed to convert data into numeric form and recreate the test/train datasets.
```

##SVM Model Using Only Significant Variables

```{r}
#USING ONLY THE VARIABLES THAT WORKED IN THE 90% LOGISTIC REGRESSION (log_all7)
train2 = subset(train1, select = c(int_corr , age_o , pf_o_sin , pf_o_sha , attr_o , fun_o , amb_o , prob_o , met_o , career_c , imprace , exercise , gaming , movies , concerts , exphappy , attr2_1 , intel2_1, match))
test2 = subset(test1, select = c(int_corr , age_o , pf_o_sin , pf_o_sha , attr_o , fun_o , amb_o , prob_o , met_o , career_c , imprace , exercise , gaming , movies , concerts , exphappy , attr2_1 , intel2_1, match))
svm2 <- ksvm(match ~ ., data = train2,
                          kernel = "rbfdot")
svm2

# predictions on testing dataset
svm2_predictions <- predict(svm2, test2)
svm2_predictions = ifelse(svm2_predictions > 0, 1, 0)
head(svm2_predictions)
table(svm2_predictions, test2$match)

# look only at agreement vs. non-agreement
# construct a vector of TRUE/FALSE indicating correct/incorrect predictions
agreement2 <- svm2_predictions == test2$match
table(agreement2)
prop.table(table(agreement2))
```

###Explanation:
```{}
In the model above, we made the SVM only use significant variables (those identified as significant with 90% confidence in our logistic regression). This was done to increase the accuracy of the model by eliminating noise.

Aside from the one above, we built several other SVM models to try and predict the likelihood of individuals matching. We attempted a vanilla model and an rbfdot modelt with all variables. These have been excluded because of their low accuracies. The best SVM model we obtained is the model seen above. Still, this SVM only achieves ~28% accuracy.
```

##Takeaway
```{}
Of all the SVM models built, the highest accuracy achieved is only 28%. Therefore, we recommend against using this model to predict match likelihood.
```


#Neural Network

##Prepare Data

```{r}
# custom normalization function
normalize <- function(x) { 
  return((x - min(x)) / (max(x) - min(x)))
}
# apply normalization to entire data frame
nums_norm = as.data.frame(lapply(nums[,-5], normalize))
nums_norm$match = nums$match

library(neuralnet)

train1 = nums_norm[1:5000,]
test1 = nums_norm[5001:6000,]
```

```{}
To prepare for our Neural Network model, we have normalized the data and created new train/test datasets.
```

##Build ANN With Only Significant Variables

```{r}
#THIS MODEL ONLY USES THE VARIABLES FROM THE 90% CONFIDENCE MODEL (From previously made numeric-only logistic regression)
match_model2 <- neuralnet(formula =  match ~ int_corr + age_o + pf_o_sin + pf_o_sha + attr_o + fun_o + prob_o + met_o +  imprace +  exercise + art + gaming + movies + concerts + music + exphappy + attr2_1 + intel2_1, data = train1, hidden = 2)

plot(match_model2)

test2 = subset(test1, select = c(int_corr, age_o , pf_o_sin , pf_o_sha , attr_o , fun_o , prob_o , met_o ,  imprace ,  exercise , art , gaming , movies , concerts , music , exphappy , attr2_1 , intel2_1))
test2$match = test1$match

#prediction for the model
model_results2 <- compute(match_model2, test2[1:18]) #every column except the result column and X
# obtain predicted strength values
predicted_match2 <- model_results2$net.result
# examine the correlation between predicted and actual values
cor(predicted_match2, test2$match)
```

```{}
Seeing as our SVM model had the best performance when using solely the signficant variables from our logistic regression, we tried this with our ANN. When using 2 hidden neurons and selecting our variables based on significance, we achieved 38% accuracy.

Note: The variables in this model were chosen using a numeric-only logistic regression thath is not shown in our report for the sake of brevity.
```

##Build Simple ANN

```{r}
# simple ANN with only a single hidden neuron
match_model1 = neuralnet(formula =  match ~ gender + condtn + order + int_corr + samerace + age_o + race_o + pf_o_att + pf_o_sin + pf_o_int + pf_o_fun + pf_o_amb + pf_o_sha + dec_o + attr_o + sinc_o + intel_o + fun_o + amb_o + shar_o + like_o + prob_o + met_o + age + race + imprace + imprelig + sports + tvsports + exercise + dining + museums + art + hiking + gaming + clubbing + reading + tv + theater + movies + concerts + music + shopping + yoga + exphappy + attr1_1 + sinc1_1 + intel1_1 + fun1_1 + amb1_1 + shar1_1 + attr2_1 + sinc2_1 + intel2_1 + fun2_1 + amb2_1 + shar2_1 + attr3_1 + sinc3_1 + fun3_1 + intel3_1 + amb3_1  , data = train1, hidden = 1)

plot(match_model1)

#prediction for the model
model_results1 = compute(match_model1, test1[2:63]) #every column except the result column and X
#obtain predicted strength values
predicted_match1 = model_results1$net.result
#examine the correlation between predicted and actual values
cor(predicted_match1, test1$match)
```

```{}
The above neural network model utilizes 1 hidden neuron and results in a 59% accuracy. This model uses all available variables and was the most basic neural network we prepared. This performs better than the first neural network that uses only select variables.
```

##Takeaway
```{}
While our neural network model is not perfect, it certainly can help us formulate hypothesis of the likelihood of matching with a randomly selected potential partner. Still, we do not recommend solely relying on this model's prediction. Combining this prediction with that of other models would strengthen a prediction.
```

